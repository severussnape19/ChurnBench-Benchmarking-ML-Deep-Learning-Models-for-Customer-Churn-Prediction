1)What is ReLU?
ReLU is a fundamental topic in deep learning and neural networks. It stands for *Rectified Linear unit* and it is an
activation function.

if x > 0, then ReLU(x) = x
if x <=0, then ReLU(x) = 0

Here, x is the input value from the features.

2) What is SMOTE?
"Synthetic minority over-sampling technique"

SMOTE is used for imbalanced datasets.
-> Picks a sample from minority class
-> Finds its k-nearest neigbhour
-> Creates synthetic samples between them

We do this to prevent the model being biased towards the majority class. The model might predict everything to be of the majority 
class and still get 95% accuracy but it isn't useful as it never learns the patterns from the minority class.

3) Labeled Encoding vs OneHotEncoding
Labeled encoding converts each category/class into a unique integer

OneHot Encoding converts each class label into a Binary vector

Batch Normalization (BatchNorm)
Normalizes the inputs to each layer during training, to stabilize and accelerate learning.

Dropout
Randomly disables (drops) a fraction of neurons during training to prevent overfitting.
